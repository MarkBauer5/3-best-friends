# -*- coding: utf-8 -*-
"""Real and Fake Faces(Swin Transformer V2)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/real-and-fake-faces-swin-transformer-v2-626ab72f-2a4b-41d7-a3d7-da5f9b8c3fe4.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240405/auto/storage/goog4_request%26X-Goog-Date%3D20240405T220857Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Deb5ab17fcfd8d6a283adad20f6c91724f6c39ad10e06b2ee4b51d0cbb1e3cc0070b8bf3c4d8293e6406e357d74a7b084a2db2619e9fbc9441595f39cbdcac03d8de6ed93bddbbe999d8275756c525d97bc6c78aeab821d48093991c52de6db7ffd24423166271549256f8a5a2e503280c71eca5b3caad86ca29da0e277fcaa3411fd25a5f24e9b8410284b94c0c8af753882dfc4bd1b7cf082fe47c0397feb3a91e9e16273aa33f0700709ce6104b34174ef56eeda86d097033b613da89a76bf82c4d4348d93a812c4cf2b2a1af560cd0c64d9f7f8bd8f017622b628498c780ff0742f64456fee94ebad6823b73f180caaa67280ec3627e6c995997fbe9f5359
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.


# %config Completer.use_jedi = False
import gc
import random
import os

import torch
import warnings
import torchvision
import numpy as np
from PIL import Image
from pathlib import Path
from torch import optim
from torchvision import models
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset, sampler
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from pytorch_grad_cam import GradCAM, GradCAMPlusPlus
from pytorch_grad_cam.utils.image import show_cam_on_image
from captum.attr import Saliency
from captum.attr import DeepLift
from captum.attr import Occlusion
from captum.attr import IntegratedGradients
from captum.attr import visualization as viz

from swin_transformer_v2 import SwinTransformerV2

if __name__ == "__main__":
# set random seeds to make results reproducible
    def seed_everything(seed=42):
        os.environ['PYTHONHASHSEED'] = str(seed)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

    SEED = 42
    seed_everything(SEED)

    warnings.filterwarnings("ignore")
    plt.rcParams.update({'axes.titlesize': 20})

    """## 1.3 Arguments
    
    We put hyperparameter together for easy modification.
    """

    class Args:
        def __init__(self) -> None:
            # data arguments
            self.num_classes = 2
            self.img_size = 256
            self.num_train_data = 10000
            self.num_test_data = 2000
            self.dataset_path = "../../../datasets/140k Real vs Fake/real_vs_fake/real-vs-fake/"

            # training arguments
            self.learning_rate =  1e-4
            self.epochs = 10
            self.scheduler = True
            self.sch_step_size = 2
            self.sch_gamma = 0.1

            # model arguments
            self.drop_path_rate = 0.2
            self.embed_dim = 96
            self.depths = (2, 2, 6, 2)
            self.num_heads = (3, 6, 12, 24)
            self.window_size = 16
            self.load_model_path = "../model/swinv2_tiny_patch4_window16_256.pth"
            self.save_model_path = "../model/swinv2_tiny_patch4_window16_256.pth"

            # output arguments
            self.output_path = "../output/"

    args = Args()

    """# 2. Data
    
    ## 2.1 Augmentation
    
    Take some augmentation actions on the images to improve training effectiveness.
    """

    train_augmentations = transforms.Compose([
        transforms.RandomResizedCrop(args.img_size, scale=(0.6, 1.0), ratio=(3./ 4., 4. / 3.)),
        transforms.RandomHorizontalFlip(0.5),
        # transforms.RandomVerticalFlip(0.1),  # no VerticalFlip
        transforms.RandomPerspective(distortion_scale=0.2, p=0.2),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.1, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    test_augmentations = transforms.Compose([
        transforms.Resize(args.img_size),
        transforms.CenterCrop(args.img_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ])
    basic_augmentations = transforms.Compose([
        transforms.Resize(args.img_size),
        transforms.CenterCrop(args.img_size),
        transforms.ToTensor()
    ])

    """## 2.2 Read and split
    
    Read the datasets and split it into training and testing sets. It should be noted that due to the large size of the original dataset, we will only randomly select a portion of the data to complete our task.
    """

    # read train and test dataset
    train_dataset = datasets.ImageFolder(root=args.dataset_path + "train/", transform=train_augmentations)
    test_dataset = datasets.ImageFolder(root=args.dataset_path + "test/", transform=test_augmentations)

    # select a subset of the dataset
    train_fake_all_indices = np.arange(len(train_dataset) / 2, dtype=np.int32)
    train_fake_indices = np.random.choice(train_fake_all_indices, size=int(args.num_train_data / 2), replace=False)
    train_real_indices = train_fake_indices + int(len(train_dataset) / 2)
    train_indices = np.append(train_fake_indices, train_real_indices)

    test_fake_all_indices = np.arange(len(test_dataset) / 2, dtype=np.int32)
    test_fake_indices = np.random.choice(test_fake_all_indices, size=int(args.num_test_data / 2), replace=False)
    test_real_indices = test_fake_indices + int(len(test_dataset) / 2)
    test_indices = np.append(test_fake_indices, test_real_indices)

    train_sampler = sampler.SubsetRandomSampler(train_indices)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, num_workers=2, sampler=train_sampler)
    test_sampler = sampler.SubsetRandomSampler(test_indices)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, num_workers=2, sampler=test_sampler)

    classes = train_dataset.classes
    class_to_idx = train_dataset.class_to_idx
    idx_to_class = dict(zip(class_to_idx.values(), class_to_idx.keys()))

    """## 2.3 Visualization
    
    Visualize the images we read in, as well as the augmented data.
    """

    raw_dataset = datasets.ImageFolder(root=args.dataset_path + "train/", transform=basic_augmentations)
    valid_dataset = datasets.ImageFolder(root=args.dataset_path + "valid/", transform=basic_augmentations)

    # we randomly select real and fake face images
    indices = [random.randint(0, len(train_dataset)) for i in range(16)]

    # show raw training data
    figure = plt.figure(figsize=(16, 16))
    for i in range(16):
        index = indices[i]
        img = raw_dataset[index][0].permute(1, 2, 0)
        label = idx_to_class[raw_dataset[index][1]]
        figure.add_subplot(4, 4, i + 1)
        plt.title(label)
        plt.axis("off")
        plt.imshow(img)

    # show augmented training data
    figure = plt.figure(figsize=(16, 16))
    for i in range(16):
        index = indices[i]
        img = train_dataset[index][0].permute(1, 2, 0)
        label = idx_to_class[train_dataset[index][1]]
        figure.add_subplot(4, 4, i + 1)
        plt.title(label)
        plt.axis("off")
        plt.imshow(img)

    """# 3. Model
    
    ## 3.1 Network
    
    We will use pre-trained swin-transformer V2 model.
    """

    # load pretrained model
    model = SwinTransformerV2(img_size=args.img_size,
                              drop_path_rate=args.drop_path_rate,
                              embed_dim=args.embed_dim,
                              depths=args.depths,
                              num_heads=args.num_heads,
                              window_size=args.window_size)
    state_dict = torch.load(args.load_model_path)
    model.load_state_dict(state_dict["model"])

    # change the last linear layer to fit our classification problem
    model.head = torch.nn.Linear(model.head.in_features, args.num_classes)

    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    model = model.to(device)

    """## 3.2 Optimizer"""

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)

    if args.scheduler:
        scheduler = optim.lr_scheduler.StepLR(optimizer,
                                              step_size=args.sch_step_size,
                                              gamma=args.sch_gamma)

    """## 3.3 Loss function"""

    loss_fn = torch.nn.CrossEntropyLoss()

    """# 4. Train
    
    Start training the model and record the intermediate results, including loss, accuracy, precision, recall and f1-score.
    """

    train_acc, test_acc = [], []
    train_precision, test_precision = [], []
    train_recall, test_recall = [], []
    train_f1, test_f1 = [], []
    train_loss, test_loss = [], []

    class LossBuffer:
        """
        We hope to record all losses over a period of time
        and calculate their average value,
        which is smooth and does not have too much jitter.
        In fact, we don't need to record the entire array,
        only the current average and number of records are enough.
        """

        def __init__(self, mean=0, n=0):
            self.mean = mean
            self.n = n

        def add(self, num):
            self.mean = (self.mean * self.n + num) / (self.n + 1)
            self.n += 1

    def train(model, dataloader, epoch):
        model.train()
        correct, cursum = 0, 0
        loop = tqdm(dataloader, total=len(dataloader))
        y_true, y_pred = [], []
        loss_buffer = LossBuffer()
        for idx, (data, label) in enumerate(loop):
            data, label = data.to(device), label.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            y_true.extend(label.cpu())
            y_pred.extend(pred.cpu())
            acc = accuracy_score(y_true, y_pred)

            optimizer.zero_grad()
            loss = loss_fn(output, label)
            loss.backward()
            optimizer.step()
            loss_buffer.add(loss.item())

            loop.set_description(f"[Epoch {epoch}/{args.epochs}]")
            loop.set_postfix(LOSS="{:.6f}".format(loss_buffer.mean), ACC="{:.2f}%".format(100 * acc))

        if args.scheduler:
            scheduler.step()

        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')
        torch.save(model.state_dict(), args.save_model_path)

        train_acc.append(acc)
        train_precision.append(precision)
        train_recall.append(recall)
        train_f1.append(f1)
        train_loss.append(loss_buffer.mean)

    def test(model, dataloader, epoch):
        model.eval()
        correct = 0
        y_true, y_pred = [], []
        with torch.no_grad():
            total_len = len(dataloader.dataset)
            loss_buffer = LossBuffer()
            for idx, (data, label) in enumerate(dataloader):
                data, label = data.to(device), label.to(device)
                output = model(data)
                pred = output.argmax(dim=1)
                y_true.extend(label.cpu())
                y_pred.extend(pred.cpu())
                loss = loss_fn(output, label)
                loss_buffer.add(loss.item())
            acc = accuracy_score(y_true, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')

        print("\n" + "-" * 60)
        print("[Epoch {}/{}]:  Test -> LOSS: {:.6f}  |  Accuracy: {:.2f}%".format(epoch, args.epochs, loss_buffer.mean, 100 * acc))
        print("-" * 60 + "\n")

        test_acc.append(acc)
        test_precision.append(precision)
        test_recall.append(recall)
        test_f1.append(f1)
        test_loss.append(loss_buffer.mean)

    # for epoch in range(1, args.epochs + 1):
    #     train(model, train_loader, epoch)
    #     test(model, test_loader, epoch)

    """# 5. Visualization
    
    ## 5.1 Print
    """

    dic = {
        "train_loss"     : train_loss,
        "test_loss"      : test_loss,
        "train_acc"      : train_acc,
        "test_acc"       : test_acc,
        "train_precision": train_precision,
        "test_precision" : test_precision,
        "train_recall"   : train_recall,
        "test_recall"    : test_recall,
        "train_f1"       : train_f1,
        "test_f1"        : test_f1,
    }

    # print and save results
    for key, value in dic.items():
        print(key + ": " + str(value), "\n")
        np.savetxt(args.output_path + key + ".txt", value)

    """## 5.2 Plot"""

    # # plot loss
    # plt.plot(np.arange(1, args.epochs + 1), np.array(train_loss), 'go-')
    # plt.plot(np.arange(1, args.epochs + 1), np.array(test_loss), 'ro-')
    # plt.xticks(np.arange(2, args.epochs + 1, 2))
    # plt.title("Loss")
    # plt.grid(True)
    # plt.legend(["train", "test"], loc="upper right")
    # plt.savefig("../output/Loss.png", dpi=600)
    #
    # figure = plt.figure(figsize=(16,16))
    #
    # # plot accuracy
    # figure.add_subplot(2, 2, 1)
    # plt.plot(np.arange(1, args.epochs + 1), np.array(train_acc), 'go-')
    # plt.plot(np.arange(1, args.epochs + 1), np.array(test_acc), 'ro-')
    # plt.xticks(np.arange(2, args.epochs + 1, 2))
    # plt.title("Accuracy")
    # plt.grid(True)
    # plt.legend(["train", "test"], loc="lower right")
    # plt.savefig("../output/Accuracy.png", dpi=600)
    #
    # # plot precision
    # figure.add_subplot(2, 2, 2)
    # plt.plot(np.arange(1, args.epochs + 1), np.array(train_precision), 'go-')
    # plt.plot(np.arange(1, args.epochs + 1), np.array(test_precision), 'ro-')
    # plt.xticks(np.arange(2, args.epochs + 1, 2))
    # plt.title("Precision")
    # plt.grid(True)
    # plt.legend(["train", "test"], loc="lower right")
    # plt.savefig("../output/Precision.png", dpi=600)
    #
    # # plot recall
    # figure.add_subplot(2, 2, 3)
    # plt.plot(np.arange(1, args.epochs + 1), np.array(train_recall), 'go-')
    # plt.plot(np.arange(1, args.epochs + 1), np.array(test_recall), 'ro-')
    # plt.xticks(np.arange(2, args.epochs + 1, 2))
    # plt.title("Recall")
    # plt.grid(True)
    # plt.legend(["train", "test"], loc="lower right")
    # plt.savefig("../output/Recall.png", dpi=600)
    #
    # # plot F1-score
    # figure.add_subplot(2, 2, 4)
    # plt.plot(np.arange(1, args.epochs + 1), np.array(train_f1), 'go-')
    # plt.plot(np.arange(1, args.epochs + 1), np.array(test_f1), 'ro-')
    # plt.xticks(np.arange(2, args.epochs + 1, 2))
    # plt.title("F1-score")
    # plt.grid(True)
    # plt.legend(["train", "test"], loc="lower right")
    # plt.savefig("../output/F1-score.png", dpi=600)

    """## 5.3 Classification prediction
    
    Visualize the prediction results of real and fake faces in the format of: "true label -> predicted label".
    """

    def showprediction(indices):
        figure = plt.figure(figsize=(16, 16))
        for i in range(16):
            index = indices[i]
            data, true_label = valid_dataset[index]
            true_label = idx_to_class[true_label]
            img = data.permute(1, 2, 0)
            data = data.unsqueeze(0).to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            pred_label = idx_to_class[pred.item()]
            figure.add_subplot(4, 4, i + 1)
            plt.title(true_label + " -> " + pred_label)
            plt.axis("off")
            plt.imshow(img)

    indices = [random.randint(0, len(valid_dataset)) for i in range(16)]
    showprediction(indices)

    indices = [random.randint(0, len(valid_dataset)) for i in range(16)]
    showprediction(indices)

    """## 5.4 Grad-CAM
    
    Use pytorch-grad-cam to visualize cam. Observing areas that have a significant impact on prediction results.
    """

    target_layer = [model.norm]

    index = 4
    data, label = valid_dataset[index]
    data = data.unsqueeze(0).to(device)
    rgb_img = np.transpose(data.cpu().numpy().squeeze(0), (1,2,0))

    def rt(tensor, height=8, width=8):
        result = tensor.reshape(tensor.size(0), height, width, tensor.size(2))
        result = result.transpose(2, 3).transpose(1, 2)
        return result

    cam = GradCAMPlusPlus(model=model, target_layers=target_layer, reshape_transform=rt)
    grayscale_cam = cam(input_tensor=data, aug_smooth=True)[0]

    cam_img = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)
    plt.title(idx_to_class[label])
    plt.axis("off")
    plt.imshow(cam_img);

    """## 5.5 Captum
    
    Use facebook captum to visualize feature map. It may help us to locate those pixels that affect the results.
    """

    model.zero_grad()
    model.eval();

    index = 5
    data, label = valid_dataset[index]
    data = data.unsqueeze(0).to(device)
    data.requires_grad = True

    """### 5.5.1 Original Image"""

    original_image = np.transpose(data.squeeze(0).cpu().detach().numpy(), (1, 2, 0))

    viz.visualize_image_attr(None, original_image, method="original_image", title="Original Image");

    """### 5.5.2 Overlayed Gradient Magnitudes
    
    Make sure you have enough cuda memory to run the following commented code.
    """

    # saliency = Saliency(model)
    # grads = saliency.attribute(data, target=label)
    # grads = np.transpose(grads.squeeze(0).cpu().detach().numpy(), (1, 2, 0))

    # viz.visualize_image_attr(grads, original_image, method="blended_heat_map", sign="absolute_value",
    #                          show_colorbar=True, title="Overlayed Gradient Magnitudes");

    # del saliency, grads
    # for i in range(10):
    #     torch.cuda.empty_cache()

    """### 5.5.3 Overlayed Integrated Gradients
    
    Make sure you have enough cuda memory to run the following commented code.
    """

    # ig = IntegratedGradients(model)
    # attr_ig = ig.attribute(data, target=label, baselines=data * 0)
    # attr_ig = np.transpose(attr_ig.squeeze(0).cpu().detach().numpy(), (1, 2, 0))

    # viz.visualize_image_attr(attr_ig, original_image, method="blended_heat_map", sign="all",
    #                              show_colorbar=True, title="Overlayed Integrated Gradients");

    # del ig, attr_ig
    # for i in range(10):
    #     torch.cuda.empty_cache()

    """### 5.5.4 Overlayed DeepLift
    
    Make sure you have enough cuda memory to run the following commented code.
    """

    # dl = DeepLift(model)
    # attr_dl = dl.attribute(data, target=label, baselines=data * 0)
    # attr_dl = np.transpose(attr_dl.squeeze(0).cpu().detach().numpy(), (1, 2, 0))

    # viz.visualize_image_attr(attr_dl, original_image, method="blended_heat_map", sign="all",
    #                          show_colorbar=True, title="Overlayed DeepLift");

    # del dl, attr_dl
    # for i in range(10):
    #     torch.cuda.empty_cache()

    """### 5.5.5 Occlusion-based attribution"""

    occlusion = Occlusion(model)
    attributions_occ = occlusion.attribute(data, strides=(3,8,8), target=label, sliding_window_shapes=(3,15,15), baselines=0)
    attributions_occ = np.transpose(attributions_occ.squeeze(0).cpu().detach().numpy(), (1, 2, 0))
    viz.visualize_image_attr_multiple(attributions_occ, original_image, ["original_image", "heat_map"],
                                      ["all", "positive"], titles=["Original Image", "Heat Map"],
                                      show_colorbar=True, outlier_perc=2);

    occlusion = Occlusion(model)
    attributions_occ = occlusion.attribute(data, strides=(3,35,35), target=label, sliding_window_shapes=(3,50,50), baselines=0)
    attributions_occ = np.transpose(attributions_occ.squeeze(0).cpu().detach().numpy(), (1, 2, 0))
    viz.visualize_image_attr_multiple(attributions_occ, original_image, ["original_image", "heat_map"],
                                      ["all", "positive"], titles=["Original Image", "Heat Map"],
                                      show_colorbar=True, outlier_perc=2);

    del occlusion, attributions_occ
    for i in range(10):
        torch.cuda.empty_cache()