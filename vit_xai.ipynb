{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pytorch_grad_cam import GradCAM, \\\n",
    "    ScoreCAM, \\\n",
    "    GradCAMPlusPlus, \\\n",
    "    AblationCAM, \\\n",
    "    XGradCAM, \\\n",
    "    EigenCAM, \\\n",
    "    EigenGradCAM, \\\n",
    "    LayerCAM, \\\n",
    "    FullGrad\n",
    "\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    preprocess_image\n",
    "from pytorch_grad_cam.ablation_layer import AblationLayerVit\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\anaconda3\\envs\\CS543Env\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisualizableVIT' object has no attribute 'blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectedData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvisualizableVIT-0_Epoch25_Batch64_LR0.001_Momentum0.9\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Replace with model path\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# modelChildren = list(model.children())\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m target_layers \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnorm1] \u001b[38;5;66;03m# Change which layer we look at, it's a lot easier to examine modelChildren in the debugger\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Custom CNN, this one does work\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# model = torch.load('CollectedData\\Models\\DWSConvNet3_learnedPoolingHwy-0_Epoch25_Batch64_LR0.001_Momentum0.9') # Replace with model path\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# modelChildren = list(model.children())\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# print(modelChildren) # Uncomment me at your own risk... It's a long model\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# target_layers = [modelChildren[3]] # Change which layer we look at, it's a lot easier to examine modelChildren in the debugger\u001b[39;00m\n\u001b[0;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\anaconda3\\envs\\CS543Env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VisualizableVIT' object has no attribute 'blocks'"
     ]
    }
   ],
   "source": [
    "def reshape_transform(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
    "                                      height, width, tensor.size(2))\n",
    "\n",
    "    # Bring the channels to the first dimension,\n",
    "    # like in CNNs.\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\" python vit_gradcam.py --image-path <path_to_image>\n",
    "    Example usage of using cam-methods on a VIT network.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # REAL ARGUMENTS FOR REAL MEN\n",
    "    method = 'gradcam++'\n",
    "    use_cuda = True\n",
    "    image_path = r\"datasets\\140k Real vs Fake\\real_vs_fake\\real-vs-fake\\train\\fake\\00B4R41FLE.jpg\"\n",
    "    eigen_smooth = True\n",
    "    aug_smooth = True\n",
    "    \n",
    "    methods = \\\n",
    "        {\"gradcam\": GradCAM,\n",
    "         \"scorecam\": ScoreCAM,\n",
    "         \"gradcam++\": GradCAMPlusPlus,\n",
    "         \"ablationcam\": AblationCAM,\n",
    "         \"xgradcam\": XGradCAM,\n",
    "         \"eigencam\": EigenCAM,\n",
    "         \"eigengradcam\": EigenGradCAM,\n",
    "         \"layercam\": LayerCAM,\n",
    "         \"fullgrad\": FullGrad}\n",
    "\n",
    "\n",
    "    if method not in list(methods.keys()):\n",
    "        raise Exception(f\"method should be one of {list(methods.keys())}\")\n",
    "\n",
    "    # This one does work, just uncomment the reshape_transform argument in the cam methods below.\n",
    "    # Results still look a bit odd\n",
    "    # model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "    # target_layers = [model.blocks[-1].norm1]\n",
    "\n",
    "    # NO IDEA HOW TO MAKE THIS ONE WORK\n",
    "    # model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=False, num_classes=2)\n",
    "    # model.load_state_dict(torch.load('CollectedData\\Models\\swinNewInit-0_Epoch25_Batch64_LR0.001_Momentum0.9'))\n",
    "    # target_layers = [model.layers[-1].blocks[-1].norm1]\n",
    "    \n",
    "    # Very stripey vertically, may be passing in the wrong layer?\n",
    "    # model = torch.load('CollectedData\\Models\\swinNewInit-0_Epoch25_Batch64_LR0.001_Momentum0.9')\n",
    "    # target_layers = [model.layers[-1].blocks[-1].norm1]\n",
    "\n",
    "    # Very stripey vertically, may be passing in the wrong layer?\n",
    "    # model = timm.models.swin_base_patch4_window7_224_in22k(pretrained=True, num_classes = 2)\n",
    "    # target_layers = [model.layers[-1].blocks[-1].norm1]\n",
    "\n",
    "\n",
    "    # Visualizable VIT, this one should work but IDK which layer to use\n",
    "    # model = torch.load(r'CollectedData\\Models\\visualizableVIT-0_Epoch25_Batch64_LR0.001_Momentum0.9') # Replace with model path\n",
    "    # modelChildren = list(model.children())\n",
    "    # target_layers = [] # Change which layer we look at, it's a lot easier to examine modelChildren in the debugger\n",
    "\n",
    "\n",
    "\n",
    "    # Custom CNN, this one does work\n",
    "    model = torch.load('CollectedData\\Models\\DWSConvNet3_learnedPoolingHwy-0_Epoch25_Batch64_LR0.001_Momentum0.9') # Replace with model path\n",
    "    modelChildren = list(model.children())\n",
    "    print(modelChildren) # Uncomment me at your own risk... It's a long model\n",
    "    target_layers = [modelChildren[3]] # Change which layer we look at, it's a lot easier to examine modelChildren in the debugger\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "\n",
    "    if method not in methods:\n",
    "        raise Exception(f\"Method {method} not implemented\")\n",
    "\n",
    "    if method == \"ablationcam\":\n",
    "        cam = methods[\"ablationcam\"](model=model,\n",
    "                                   target_layers=target_layers,\n",
    "                                #    use_cuda=use_cuda,\n",
    "                                #    reshape_transform=reshape_transform,\n",
    "                                   ablation_layer=AblationLayerVit())\n",
    "    else:\n",
    "        cam = methods[method](model=model,\n",
    "                                   target_layers=target_layers,\n",
    "                                #    use_cuda=use_cuda,\n",
    "                                #    reshape_transform=reshape_transform\n",
    "                                )\n",
    "\n",
    "    rgb_img = cv2.imread(image_path, 1)[:, :, ::-1]\n",
    "    rgb_img = cv2.resize(rgb_img, (224, 224))\n",
    "    rgb_img = np.float32(rgb_img) / 255\n",
    "    input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n",
    "                                    std=[0.5, 0.5, 0.5])\n",
    "\n",
    "    # If None, returns the map for the highest scoring category.\n",
    "    # Otherwise, targets the requested category.\n",
    "    targets = None\n",
    "\n",
    "    # AblationCAM and ScoreCAM have batched implementations.\n",
    "    # You can override the internal batch size for faster computation.\n",
    "    cam.batch_size = 32\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=input_tensor,\n",
    "                        targets=targets,\n",
    "                        eigen_smooth=eigen_smooth,\n",
    "                        aug_smooth=aug_smooth)\n",
    "\n",
    "    # Here grayscale_cam has only one image in the batch\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n",
    "    outFileName = f'{method}_cam.jpg'\n",
    "    cv2.imwrite(outFileName, cam_image)\n",
    "    \n",
    "    print(f'Exported result to {outFileName}!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS543Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
